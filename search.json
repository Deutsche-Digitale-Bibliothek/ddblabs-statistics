[
  {
    "objectID": "pages/notebooks.html",
    "href": "pages/notebooks.html",
    "title": "Notebooks",
    "section": "",
    "text": "Historischer Stand:\n    \n      Aktuell\n    \n    \n  \n\nHier sind die einzelnen Notebooks mit direkten Start-Links aufgelistet.\n\nAuffällige Mehrfachwerte je Provider inspection-invalid_multiple_facets_values.ipynb\n\nSeite Download GitHub nbviewer VS Code (Web) Colab Binder VS Code (lokal) GitHub Desktop\n\n\n\nDatenpartner mit Anzahl der Objekte pro Bundesland statistic-federal_state.ipynb\n\nSeite Download GitHub nbviewer VS Code (Web) Colab Binder VS Code (lokal) GitHub Desktop"
  },
  {
    "objectID": "inspection-invalid_multiple_facets_values.html",
    "href": "inspection-invalid_multiple_facets_values.html",
    "title": "Auffällige Mehrfachwerte je Provider",
    "section": "",
    "text": "Einige Felder können je dataprovider_id mehrere unterschiedliche Werte haben (z.B. durch Datenmischung). Dafür wird ein Solr JSON-Facet genutzt und anschließend client-seitig auf nvals &gt; 1 sowie 32-stellige IDs gefiltert. Hinweis: unique(...) ist in Solr oft ein schneller, näherungsweiser Distinct-Count (für das Finden von Kandidaten meist ausreichend).\nQuelltext anzeigen\nimport requests\nimport pandas as pd\nimport json\nQuelltext anzeigen\n# Organization-Daten (ddb-institution) laden -&gt; DataFrame\nurl_org = \"https://api.deutsche-digitale-bibliothek.de/2/search/index/organization/select\"\nparams_org = {\n    \"q\": \"type:ddb-institution OR type:ddb-aggregator\",\n    \"fl\": \"id,label\",\n    \"rows\": 10000,\n    \"wt\": \"json\",\n}\n\nresp_org = requests.get(url_org, params=params_org, timeout=300)\nresp_org.raise_for_status()\ndata_org = resp_org.json()\n\ndocs = data_org.get(\"response\", {}).get(\"docs\", [])\n\ndef _as_list(value):\n    if value is None:\n        return []\n    # Solr liefert bei *_fct i.d.R. Listen; zur Sicherheit wird auch ein Scalar zu einer Liste normalisiert\n    return value if isinstance(value, list) else [value]\n\ndf_org = pd.DataFrame({\n    \"id\": [d.get(\"id\") for d in docs],\n    \"label\": [_as_list(d.get(\"label\")) for d in docs],\n})\n\ndf_org\n\n\n\n\n\n\n\n\n\nid\nlabel\n\n\n\n\n0\n5IMBCSZN7ZVL77HVH4GKX3GXNPD6X2NO\n[Deutsche Fotothek]\n\n\n1\nDWIVLJW5JBO7X4XRO2Y7PCGZSHLTUS6F\n[Digitales Kunst- und Kulturarchiv Düsseldorf ...\n\n\n2\n3IJSQ3OFJZCXLJLKDTIQIPYYS5TUMJLB\n[Bayerische Julius-Maximilians-Universität Wür...\n\n\n3\nCLELTYIEQZZMAYTTNY6TN4BV3KH6OXMT\n[Franckesche Stiftungen]\n\n\n4\n2ZIDL3W7JH7TC5AX63VFNLA5S3YJTQIY\n[Technische Universität Dresden]\n\n\n...\n...\n...\n\n\n4944\n4N352HQISWFBP37MFWD3PQSJIVCZ2WTY\n[Stadtarchiv Neustadt in Holstein]\n\n\n4945\nQGCRF5ETJ7S746OUN3EWT76HLCI2EJNF\n[Stadtarchiv Holzminden]\n\n\n4946\n5XW2AKKSDPK3RF5NXIFRV4WOYOVF5UFW\n[Kreisarchiv des Landkreises Verden]\n\n\n4947\n6RZWY2PNOT35GHAV3V5A3WOOMMFLMCUA\n[Deutsches Adelsarchiv]\n\n\n4948\nXT7AWKQ3YOZ4MPWNRFOZTDRYW6SESIK3\n[Gemeindearchiv Schauenburg]\n\n\n\n\n4949 rows × 2 columns\nQuelltext anzeigen\nurl = \"https://api.deutsche-digitale-bibliothek.de/2/search/index/search/select\"\nparams = {\n    \"q\": \"*:*\",\n    \"rows\": 0,\n    \"wt\": \"json\",\n    \"json.facet\": json.dumps({\n        \"providers\": {\n            \"type\": \"terms\",\n            \"field\": \"dataprovider_id\",\n            \"limit\": -1,\n            \"mincount\": 1,\n            \"facet\": {\n                # distinct count (i.d.R. HLL/approx, dafür schnell)\n                \"nvals\": \"unique(dataprovider_fct)\"\n            },\n            \"sort\": \"nvals desc\"\n        }\n    })\n}\n\nj = requests.get(url, params=params, timeout=300).json()\nbuckets = j[\"facets\"][\"providers\"][\"buckets\"]\n\n# Nur auffällige Fälle\nauffaellig = [b for b in buckets if b.get(\"nvals\", 0) &gt; 1]\n\n# -&gt; DataFrame\ndf_auffaellig = pd.DataFrame([\n    {\n        \"dataprovider_id\": str(b.get(\"val\", \"\")),\n        \"dataprovider_fct_nvalues\": int(b.get(\"nvals\", 0) or 0),\n    }\n    for b in auffaellig\n])\n\n# nur 32-stellige IDs behalten\ndf_auffaellig = df_auffaellig[df_auffaellig[\"dataprovider_id\"].str.len() == 32].copy()\n\n# Label aus df_org anreichern (falls df_org existiert)\ndf_auffaellig = (\n    df_auffaellig\n    .merge(df_org[[\"id\", \"label\"]], left_on=\"dataprovider_id\", right_on=\"id\", how=\"left\")\n    .drop(columns=[\"id\"])\n)\n\ndf_auffaellig\n\n\n\n\n\n\n\n\n\ndataprovider_id\ndataprovider_fct_nvalues\nlabel\n\n\n\n\n0\n7A5GWWGTIAUDXLN6JNTFKKZUM27GCGGI\n2\n[Staatliche Kunstsammlungen Dresden. GRASSI Mu...\n\n\n1\nZCXCMB7WXARQK27QKZY6ZQSH23D4YEOA\n2\n[Queer*Feministische Bibliothek und Archiv LIE..."
  },
  {
    "objectID": "inspection-invalid_multiple_facets_values.html#variante-mehrfachwerte-in-sector_fct",
    "href": "inspection-invalid_multiple_facets_values.html#variante-mehrfachwerte-in-sector_fct",
    "title": "Auffällige Mehrfachwerte je Provider",
    "section": "Variante: Mehrfachwerte in sector_fct",
    "text": "Variante: Mehrfachwerte in sector_fct\nGleiche Logik wie oben, aber für sector_fct (Sektor-Klassifikation). Ergebnis: df_auffaellig, angereichert mit label aus df_join.\n\n\nQuelltext anzeigen\nurl = \"https://api.deutsche-digitale-bibliothek.de/2/search/index/search/select\"\nparams = {\n    \"q\": \"*:*\",\n    \"rows\": 0,\n    \"wt\": \"json\",\n    \"json.facet\": json.dumps({\n        \"providers\": {\n            \"type\": \"terms\",\n            \"field\": \"dataprovider_id\",\n            \"limit\": -1,\n            \"mincount\": 1,\n            \"facet\": {\n                # distinct count (i.d.R. HLL/approx, dafür schnell)\n                \"nvals\": \"unique(sector_fct)\"\n            },\n            \"sort\": \"nvals desc\"\n        }\n    })\n}\n\nj = requests.get(url, params=params, timeout=300).json()\nbuckets = j[\"facets\"][\"providers\"][\"buckets\"]\n\n# Nur auffällige Fälle\nauffaellig = [b for b in buckets if b.get(\"nvals\", 0) &gt; 1]\n\n# -&gt; DataFrame\ndf_auffaellig = pd.DataFrame([\n    {\n        \"dataprovider_id\": str(b.get(\"val\", \"\")),\n        \"sector_fct_nvalues\": int(b.get(\"nvals\", 0) or 0),\n    }\n    for b in auffaellig\n])\n\n# nur 32-stellige IDs behalten\ndf_auffaellig = df_auffaellig[df_auffaellig[\"dataprovider_id\"].str.len() == 32].copy()\n\n# Label aus df_org anreichern (falls df_org existiert)\ndf_auffaellig = (\n    df_auffaellig\n    .merge(df_org[[\"id\", \"label\"]], left_on=\"dataprovider_id\", right_on=\"id\", how=\"left\")\n    .drop(columns=[\"id\"])\n)\n\ndf_auffaellig\n\n\n\n\n\n\n\n\n\ndataprovider_id\nsector_fct_nvalues\nlabel\n\n\n\n\n0\nA52WOLFHOQ4JDAPIGD7O7XJTP6N7FZ5E\n2\n[Stiftung Berliner Mauer]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ddblabs-statistics",
    "section": "",
    "text": "Eine kleine Sammlung von Jupyter-Notebooks für Auswertungen zur Deutschen Digitalen Bibliothek.\nDiese Links beziehen sich auf die Nachnutzung des gesamten Repositories (nicht auf ein einzelnes Notebook)."
  },
  {
    "objectID": "index.html#hinweise",
    "href": "index.html#hinweise",
    "title": "ddblabs-statistics",
    "section": "Hinweise",
    "text": "Hinweise\n\nFür „GitHub Desktop“ und „VS Code (lokal)“ muss die Anwendung lokal installiert sein.\nKaggle-Import: im Editor File → Import Notebook → GitHub (Repository-URL: https://github.com/Deutsche-Digitale-Bibliothek/ddblabs-statistics).\nWeitere Ausführungsoptionen sind auf der Notebook-Seite pro Notebook verlinkt."
  },
  {
    "objectID": "statistic-federal_state.html",
    "href": "statistic-federal_state.html",
    "title": "Datenpartner mit Anzahl der Objekte pro Bundesland",
    "section": "",
    "text": "Dieses Notebook ruft Statistiken aus der API der Deutschen Digitalen Bibliothek ab und bereitet sie für eine Auswertung nach Bundesland auf.\n\n\nQuelltext anzeigen\nimport requests\nimport pandas as pd\nfrom datetime import datetime\nfrom IPython.display import Markdown, display\n\n# Zuerst wird aus der DDB-Suche eine Facet-Statistik zu `dataprovider_id` geladen. \n# Ergebnis dieser Zelle ist `df` mit zwei Spalten (`dataprovider_id`, `count`). \n# Zusätzlich wird auf 32-stellige Provider-IDs gefiltert, damit im DataFrame nur echte Provider-IDs stehen.\n\n# API-Call: https://api.deutsche-digitale-bibliothek.de/2/search/index/search/select?q=*%3A*&rows=0&facet=true&facet.field=dataprovider_id&facet.limit=-1&facet.sort=count&wt=json\n\nurl = \"https://api.deutsche-digitale-bibliothek.de/2/search/index/search/select\"\nparams = {\n    \"q\": \"*:*\",\n    \"rows\": 0,\n    \"facet\": \"true\",\n    \"facet.field\": \"dataprovider_id\",\n    \"facet.limit\": -1,\n    \"facet.sort\": \"count\",\n    \"wt\": \"json\",\n}\n\nresp = requests.get(url, params=params, timeout=300)\nresp.raise_for_status()\ndata = resp.json()\n\nfacet_list = data[\"facet_counts\"][\"facet_fields\"][\"dataprovider_id\"]  # [\"id1\", count1, \"id2\", count2, ...]\npairs = list(zip(facet_list[0::2], facet_list[1::2]))\n\ndf = pd.DataFrame(pairs, columns=[\"dataprovider_id\", \"count\"])\ndf[\"dataprovider_id\"] = df[\"dataprovider_id\"].astype(str)\ndf[\"count\"] = pd.to_numeric(df[\"count\"], errors=\"coerce\").fillna(0).astype(\"int64\")\n\ndf = df[df[\"dataprovider_id\"].str.len() == 32].reset_index(drop=True)\n# df\n\n# Als nächstes werden Organisationsdaten (Einrichtungen und Aggregatoren) aus dem Organisation-Index geladen. \n# Felder, die mal als Liste und mal als Einzelwert kommen, werden vereinheitlicht. \n# Ergebnis ist `df_org` mit Metadaten wie `label`, `type`, `sector_fct`, `city_de_fct` und `state_de_fct`.\n# \n# API-Call: https://api.deutsche-digitale-bibliothek.de/2/search/index/organization/select?q=type:ddb-institution%20OR%20type:ddb-aggregator&fl=id,label,type,sector_fct,city_de_fct,state_de_fct&rows=10000&wt=json\n\nurl_org = \"https://api.deutsche-digitale-bibliothek.de/2/search/index/organization/select\"\nparams_org = {\n    \"q\": \"type:ddb-institution OR type:ddb-aggregator\",\n    \"fl\": \"id,label,type,sector_fct,city_de_fct,state_de_fct\",\n    \"rows\": 10000,\n    \"wt\": \"json\",\n}\n\nresp_org = requests.get(url_org, params=params_org, timeout=300)\nresp_org.raise_for_status()\ndata_org = resp_org.json()\n\ndocs = data_org.get(\"response\", {}).get(\"docs\", [])\n\ndef _as_list(value):\n    if value is None:\n        return []\n    # Solr liefert bei *_fct i.d.R. Listen; zur Sicherheit wird auch ein Scalar zu einer Liste normalisiert\n    return value if isinstance(value, list) else [value]\n\ndf_org = pd.DataFrame({\n    \"id\": [d.get(\"id\") for d in docs],\n    \"label\": [_as_list(d.get(\"label\")) for d in docs],\n    \"type\": [d.get(\"type\") for d in docs],\n    \"sector_fct\":  [d.get(\"sector_fct\") for d in docs],\n    \"city_de_fct\": [_as_list(d.get(\"city_de_fct\")) for d in docs],\n    \"state_de_fct\": [_as_list(d.get(\"state_de_fct\")) for d in docs],\n})\n# df_org\n\n# Anschließend werden die Provider-Statistik (`df`) und die Organisationsdaten (`df_org`) zusammengeführt.\n# Damit stehen pro Einrichtung/Provider sowohl Metadaten als auch die Objektanzahl (`count`) zur Verfügung.\n# Das Ergebnis kann als Excel-Datei mit aktuellem Datum gespeichert werden (`df_join`).\n\n# Join: Provider-Facets (df) + Organisationsdaten (df_org)\ndf_org[\"id\"] = df_org[\"id\"].astype(str)\n\ndf_join = df.merge(df_org, left_on=\"dataprovider_id\", right_on=\"id\", how=\"outer\")\n\n# Für Excel: Array-Spalten lesbar als Strings serialisieren (Listen -&gt; '; '-String)\nfor col in [\"label\", \"city_de_fct\", \"state_de_fct\"]:\n    if col in df_join.columns:\n        df_join[col] = df_join[col].apply(lambda xs: \"; \".join(map(str, xs)) if isinstance(xs, list) else (\"\" if xs is None else str(xs)))\n\n# Optional: doppelte Join-Spalte entfernen und Spaltenordnung setzen\nif \"dataprovider_id\" in df_join.columns:\n    df_join = df_join.drop(columns=[\"dataprovider_id\"])\n\ndf_join = df_join[[\"id\", \"label\", \"type\", \"sector_fct\", \"city_de_fct\", \"state_de_fct\", \"count\"]]\n\n# Ergebnis in Excel-Datei speichern mit Datum\n# out_path = \"statistic-federal_state_\" + pd.Timestamp.now().strftime(\"%Y-%m-%d\") + \".xlsx\"\n# df_join.to_excel(out_path, index=False)\n\n# df_join\n\n# Im nächsten Schritt wird `df_join` nach Bundesland ausgewertet. Dafür wird das (ggf. mehrfach belegte)\n# Bundeslandfeld auf einen Wert reduziert und anschließend je Bundesland aggregiert:\n# - Anzahl eindeutiger Einrichtungen\n# - Anzahl liefernder Einrichtungen (`count &gt; 0`)\n# - Summe der Objekte (`count`)\n# \n# Ergebnis ist der DataFrame `agg`.\n\ndf_bl = df_join.loc[df_join[\"id\"].notna(), [\"id\", \"state_de_fct\", \"count\"]].copy()\ndf_bl[\"count\"] = pd.to_numeric(df_bl[\"count\"], errors=\"coerce\").fillna(0).astype(\"int64\")\n\n# state_de_fct ist nach dem Join serialisiert (\"; \"-getrennt). Für die Zuordnung wird der erste Wert verwendet.\ndf_bl[\"Bundesland\"] = (\n    df_bl[\"state_de_fct\"]\n    .fillna(\"\")\n    .astype(str)\n    .str.split(\"; \")\n    .str[0]\n    .str.strip()\n    .replace({\"\": \"Ausland\"})\n)\n\neinrichtungen = df_bl.groupby(\"Bundesland\")[\"id\"].nunique().rename(\"Einrichtungen\")\nliefernde = (\n    df_bl.loc[df_bl[\"count\"] &gt; 0]\n    .groupby(\"Bundesland\")[\"id\"]\n    .nunique()\n    .rename(\"Liefernde Einrichtungen\")\n)\nobjekte = df_bl.groupby(\"Bundesland\")[\"count\"].sum().rename(\"Objekte\")\n\nagg = (\n    pd.concat([einrichtungen, liefernde, objekte], axis=1)\n    .fillna(0)\n    .astype({\"Einrichtungen\": \"int64\", \"Liefernde Einrichtungen\": \"int64\", \"Objekte\": \"int64\"})\n    .sort_values([\"Objekte\", \"Liefernde Einrichtungen\", \"Einrichtungen\"], ascending=False)\n)\n\n# Stand: Datum/Uhrzeit der Notebook-Ausführung (lokale Zeitzone)\nstand = datetime.now().astimezone().strftime(\"%d.%m.%Y um %H:%M:%S Uhr\")\ndisplay(Markdown(f\"**Letzte Aktualisierung:** {stand}\"))\n\n# Anzeige\nagg\n\n\nLetzte Aktualisierung: 15.02.2026 um 05:00:00 Uhr\n\n\n\n\n\n\n\n\n\nEinrichtungen\nLiefernde Einrichtungen\nObjekte\n\n\nBundesland\n\n\n\n\n\n\n\nBaden-Württemberg\n589\n57\n11004229\n\n\nHessen\n382\n57\n9878446\n\n\nNordrhein-Westfalen\n883\n157\n9332725\n\n\nBerlin\n304\n108\n7199350\n\n\nSachsen\n385\n54\n7070367\n\n\nBayern\n600\n58\n6390424\n\n\nRheinland-Pfalz\n165\n29\n4553607\n\n\nSachsen-Anhalt\n144\n21\n1972997\n\n\nSchleswig-Holstein\n337\n43\n1774401\n\n\nNiedersachsen\n385\n58\n1744794\n\n\nBrandenburg\n198\n41\n1705643\n\n\nHamburg\n97\n20\n1366746\n\n\nThüringen\n282\n72\n355308\n\n\nMecklenburg-Vorpommern\n102\n15\n348860\n\n\nAusland\n13\n4\n167669\n\n\nBremen\n40\n11\n65789\n\n\nSaarland\n43\n15\n64605\n\n\n\n\n\n\n\n\n\nQuelltext anzeigen\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import FuncFormatter\n\n# Zum Schluss werden die aggregierten Werte aus `agg` als horizontale Balkendiagramme dargestellt:\n# - Objekte je Bundesland\n# - Einrichtungen vs. liefernde Einrichtungen je Bundesland\n# Die Achsenbeschriftung wird so formatiert, dass große Zahlen ohne wissenschaftliche Notation gut lesbar bleiben.\n\nplot_df = agg.iloc[::-1]  # für barh: klein -&gt; groß\n\nfig, axes = plt.subplots(1, 2, figsize=(14, max(6, 0.35 * len(plot_df))))\n\ndef _format_int_no_sci(x, pos):\n    _ = pos  # required by FuncFormatter signature\n    return f\"{x:,.0f}\".replace(\",\", \".\")\n\n# Objekte\nplot_df[\"Objekte\"].plot(kind=\"barh\", ax=axes[0], color=\"#4C78A8\")\naxes[0].set_title(\"Objekte\")\naxes[0].set_xlabel(\"Anzahl\")\naxes[0].grid(axis=\"x\", linestyle=\":\", alpha=0.4)\naxes[0].xaxis.set_major_formatter(FuncFormatter(_format_int_no_sci))\n\nplot_df[[\"Einrichtungen\", \"Liefernde Einrichtungen\"]].plot(kind=\"barh\", ax=axes[1])\naxes[1].set_title(\"Einrichtungen\")\naxes[1].set_xlabel(\"Anzahl\")\naxes[1].grid(axis=\"x\", linestyle=\":\", alpha=0.4)\naxes[1].legend(loc=\"lower right\")\n\nfig.tight_layout()\nplt.show()"
  }
]